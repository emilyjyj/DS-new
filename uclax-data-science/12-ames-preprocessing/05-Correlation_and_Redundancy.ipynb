{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy and Correlation\n",
    "\n",
    "In preparation for a principal component analysis, we look at redundancy and correlation in our dataset. In this analysis, we will focus on the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source('src/load_data-02.r')\n",
    "source('src/multiplot.r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_empty_total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_features = colnames(Filter(is.numeric, housing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = Filter(is.numeric, housing_df)\n",
    "numeric_df$SalePrice <- NULL\n",
    "numeric_features = colnames(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('rpart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(rpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy\n",
    "\n",
    "Here, we use machine learning to assess redundancy in our dataset. We iterate through each numeric feature in our dataset. For each feature, we dropped the feature from our input/featureet $X$ And use it as our target $y$ for the training of a supervised regression model. In this case, we use the shortcut for training a model on all features, `~.`, as our regression formula\n",
    "\n",
    "    this_formula = paste(feature,\"~.\")\n",
    "    fit <- rpart(data=train, formula=as.formula(this_formula))\n",
    "    \n",
    "In other words we are training a regression model where we use the remaining features to protect each individual feature. We will thus have an $R^2$ score for each numeric feature. Note,  that the `rpart` function is available as part of the `caret` library in R. This is the implementation of a decision tree. \n",
    "\n",
    "Note, that we also use machine learning best practices and perform a trainâ€“test split on our data. Each model is trained using the training data and assessed using the testing data. In this way, each model tells us if, upon removing a feature, the remaining features are able to predict the removed feature. If the remaining features are able to make this prediction, we may take the removed feature to be somewhat redundant. It is worth clarifying that this is an exploratory data analysis technique, and is not intended to be used at this time as a technique for removing features. We simply wish to understand the relationships within our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_r_2 <- function(actual, prediction) {\n",
    "    return (1 - (sum((actual-prediction)^2)/sum((actual-mean(actual))^2)))\n",
    "}\n",
    "\n",
    "calculate_r_2_for_feature <- function(data, feature) {\n",
    "    n <- nrow(data)\n",
    "    \n",
    "    train_index <- sample(seq_len(n), size = 0.8*n)\n",
    "\n",
    "    train <- data[train_index,]\n",
    "    test <- data[-train_index,]\n",
    "    \n",
    "    this_formula = paste(feature,\"~.\")\n",
    "    fit <- rpart(data=train, formula=as.formula(this_formula))\n",
    "\n",
    "    y_test <- as.vector(test[[feature]])\n",
    "    test[feature] <- NULL\n",
    "    predictions <- predict(fit, test)\n",
    "    return (calculate_r_2(y_test, predictions))\n",
    "}\n",
    "\n",
    "mean_r2_for_feature <- function (data, feature) {\n",
    "    scores = c()\n",
    "    for (i in 1:10) {\n",
    "        scores = c(scores, calculate_r_2_for_feature(data, feature))\n",
    "    }\n",
    "    \n",
    "    return (mean(scores))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_r_2_for_feature(numeric_df,'LotFrontage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (feature in numeric_features){\n",
    "    print(paste(feature, mean_r2_for_feature(numeric_df, feature)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Next, we assess correlation between our features. Correlation is a function of covariance data, which is itself a measure of linear relationships within data. In the previous section, we use a decision tree to assess redundancy. A decision tree is an information-based (non-linear) analysis. By performing this analysis using two different techniques, one linear and one non-linear, we have a more robust assessment have the underlying relationships in our data. Again, this technique is exploratory data analysis and is not intended at this time to remove features from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(digits=3)\n",
    "cor(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "cormat = cor(numeric_df)\n",
    "\n",
    "cormat[lower.tri(cormat)] <- NA\n",
    "diag(cormat) <- NA\n",
    "\n",
    "melted_cormat <- melt(cormat, na.rm = T)\n",
    "\n",
    "library(ggplot2)\n",
    "ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+\n",
    " geom_tile(color = \"white\")+\n",
    " scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n",
    "   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n",
    "   name=\"Pearson\\nCorrelation\") +\n",
    "  theme_minimal()+ \n",
    " theme(axis.text.x = element_text(angle = 45, vjust = 1, \n",
    "    size = 12, hjust = 1))+\n",
    " coord_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
